{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:56:57.279239Z","iopub.execute_input":"2022-03-02T06:56:57.279621Z","iopub.status.idle":"2022-03-02T06:56:57.305332Z","shell.execute_reply.started":"2022-03-02T06:56:57.279528Z","shell.execute_reply":"2022-03-02T06:56:57.304676Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Derived from : https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:58:14.551348Z","iopub.execute_input":"2022-03-02T06:58:14.551747Z","iopub.status.idle":"2022-03-02T06:58:14.566544Z","shell.execute_reply.started":"2022-03-02T06:58:14.551717Z","shell.execute_reply":"2022-03-02T06:58:14.565683Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = fetch_openml(data_id=41214, as_frame=True).frame\ndf","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:58:15.035314Z","iopub.execute_input":"2022-03-02T06:58:15.035717Z","iopub.status.idle":"2022-03-02T06:58:44.406685Z","shell.execute_reply.started":"2022-03-02T06:58:15.035686Z","shell.execute_reply":"2022-03-02T06:58:44.405733Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Basic EDA\ndf[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n\nprint(\n    \"Average Frequency = {}\".format(np.average(df[\"Frequency\"], weights=df[\"Exposure\"]))\n)\n\nprint(\n    \"Fraction of exposure with zero claims = {0:.1%}\".format(\n        df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() / df[\"Exposure\"].sum()\n    )\n)\n\nfig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(16, 4))\nax0.set_title(\"Number of claims\")\n_ = df[\"ClaimNb\"].hist(bins=30, log=True, ax=ax0)\nax1.set_title(\"Exposure in years\")\n_ = df[\"Exposure\"].hist(bins=30, log=True, ax=ax1)\nax2.set_title(\"Frequency (number of claims per year)\")\n_ = df[\"Frequency\"].hist(bins=30, log=True, ax=ax2)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:58:44.408523Z","iopub.execute_input":"2022-03-02T06:58:44.408755Z","iopub.status.idle":"2022-03-02T06:58:46.388420Z","shell.execute_reply.started":"2022-03-02T06:58:44.408727Z","shell.execute_reply":"2022-03-02T06:58:46.387282Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"log_scale_transformer = make_pipeline(\n    FunctionTransformer(np.log, validate=False), StandardScaler()\n)\n\nlinear_model_preprocessor = ColumnTransformer(\n    [\n        (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\n        (\"binned_numeric\", KBinsDiscretizer(n_bins=10), [\"VehAge\", \"DrivAge\"]),\n        (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),\n        (\n            \"onehot_categorical\",\n            OneHotEncoder(),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\n        ),\n    ],\n    remainder=\"drop\",\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:59:50.459408Z","iopub.execute_input":"2022-03-02T06:59:50.459700Z","iopub.status.idle":"2022-03-02T06:59:50.467784Z","shell.execute_reply.started":"2022-03-02T06:59:50.459671Z","shell.execute_reply":"2022-03-02T06:59:50.466649Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.dummy import DummyRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.33, random_state=0)\n\ndummy = Pipeline(\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", DummyRegressor(strategy=\"mean\")),\n    ]\n).fit(df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:59:52.172992Z","iopub.execute_input":"2022-03-02T06:59:52.173297Z","iopub.status.idle":"2022-03-02T06:59:53.938063Z","shell.execute_reply.started":"2022-03-02T06:59:52.173263Z","shell.execute_reply":"2022-03-02T06:59:53.937124Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_poisson_deviance\n\n\ndef score_estimator(estimator, df_test):\n    \"\"\"Score an estimator on the test set.\"\"\"\n    y_pred = estimator.predict(df_test)\n\n    print(\n        \"MSE: %.3f\"\n        % mean_squared_error(\n            df_test[\"Frequency\"], y_pred, sample_weight=df_test[\"Exposure\"]\n        )\n    )\n    print(\n        \"MAE: %.3f\"\n        % mean_absolute_error(\n            df_test[\"Frequency\"], y_pred, sample_weight=df_test[\"Exposure\"]\n        )\n    )\n\n    # Ignore non-positive predictions, as they are invalid for\n    # the Poisson deviance.\n    mask = y_pred > 0\n    if (~mask).any():\n        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n        print(\n            \"WARNING: Estimator yields invalid, non-positive predictions \"\n            f\" for {n_masked} samples out of {n_samples}. These predictions \"\n            \"are ignored when computing the Poisson deviance.\"\n        )\n\n    print(\n        \"mean Poisson deviance: %.3f\"\n        % mean_poisson_deviance(\n            df_test[\"Frequency\"][mask],\n            y_pred[mask],\n            sample_weight=df_test[\"Exposure\"][mask],\n        )\n    )\n\n\nprint(\"Constant mean frequency evaluation:\")\nscore_estimator(dummy, df_test)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T06:59:59.177314Z","iopub.execute_input":"2022-03-02T06:59:59.177574Z","iopub.status.idle":"2022-03-02T06:59:59.791409Z","shell.execute_reply.started":"2022-03-02T06:59:59.177546Z","shell.execute_reply":"2022-03-02T06:59:59.790283Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# L2 regularized model (ridge regression)\nfrom sklearn.linear_model import Ridge\n\n\nridge_glm = Pipeline(\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", Ridge(alpha=1e-6)),\n    ]\n).fit(df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:00:06.255301Z","iopub.execute_input":"2022-03-02T07:00:06.255592Z","iopub.status.idle":"2022-03-02T07:00:08.045102Z","shell.execute_reply.started":"2022-03-02T07:00:06.255559Z","shell.execute_reply":"2022-03-02T07:00:08.044408Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(\"Ridge evaluation:\")\nscore_estimator(ridge_glm, df_test)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:00:08.046490Z","iopub.execute_input":"2022-03-02T07:00:08.046846Z","iopub.status.idle":"2022-03-02T07:00:08.666299Z","shell.execute_reply.started":"2022-03-02T07:00:08.046815Z","shell.execute_reply":"2022-03-02T07:00:08.665412Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Poisson Regressor\nfrom sklearn.linear_model import PoissonRegressor\n\nn_samples = df_train.shape[0]\n\npoisson_glm = Pipeline(\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", PoissonRegressor(alpha=1e-12, max_iter=300)),\n    ]\n)\npoisson_glm.fit(\n    df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"]\n)\n\nprint(\"PoissonRegressor evaluation:\")\nscore_estimator(poisson_glm, df_test)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:00:18.050517Z","iopub.execute_input":"2022-03-02T07:00:18.051192Z","iopub.status.idle":"2022-03-02T07:00:29.068636Z","shell.execute_reply.started":"2022-03-02T07:00:18.051135Z","shell.execute_reply":"2022-03-02T07:00:29.067933Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6), sharey=True)\nfig.subplots_adjust(bottom=0.2)\nn_bins = 20\nfor row_idx, label, df in zip(range(2), [\"train\", \"test\"], [df_train, df_test]):\n    df[\"Frequency\"].hist(bins=np.linspace(-1, 30, n_bins), ax=axes[row_idx, 0])\n\n    axes[row_idx, 0].set_title(\"Data\")\n    axes[row_idx, 0].set_yscale(\"log\")\n    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n    axes[row_idx, 0].set_ylim([1e1, 5e5])\n    axes[row_idx, 0].set_ylabel(label + \" samples\")\n\n    for idx, model in enumerate([ridge_glm, poisson_glm]):\n        y_pred = model.predict(df)\n\n        pd.Series(y_pred).hist(\n            bins=np.linspace(-1, 4, n_bins), ax=axes[row_idx, idx + 1]\n        )\n        axes[row_idx, idx + 1].set(\n            title=model[-1].__class__.__name__,\n            yscale=\"log\",\n            xlabel=\"y_pred (predicted expected Frequency)\",\n        )\nplt.tight_layout()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:00:57.452302Z","iopub.execute_input":"2022-03-02T07:00:57.452563Z","iopub.status.idle":"2022-03-02T07:01:04.382401Z","shell.execute_reply.started":"2022-03-02T07:00:57.452535Z","shell.execute_reply":"2022-03-02T07:01:04.381290Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import gen_even_slices\n\n\ndef _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):\n    \"\"\"Compare predictions and observations for bins ordered by y_pred.\n\n    We order the samples by ``y_pred`` and split it in bins.\n    In each bin the observed mean is compared with the predicted mean.\n\n    Parameters\n    ----------\n    y_true: array-like of shape (n_samples,)\n        Ground truth (correct) target values.\n    y_pred: array-like of shape (n_samples,)\n        Estimated target values.\n    sample_weight : array-like of shape (n_samples,)\n        Sample weights.\n    n_bins: int\n        Number of bins to use.\n\n    Returns\n    -------\n    bin_centers: ndarray of shape (n_bins,)\n        bin centers\n    y_true_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    y_pred_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    \"\"\"\n    idx_sort = np.argsort(y_pred)\n    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\n    y_pred_bin = np.zeros(n_bins)\n    y_true_bin = np.zeros(n_bins)\n\n    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n        weights = sample_weight[idx_sort][sl]\n        y_pred_bin[n] = np.average(y_pred[idx_sort][sl], weights=weights)\n        y_true_bin[n] = np.average(y_true[idx_sort][sl], weights=weights)\n    return bin_centers, y_true_bin, y_pred_bin\n\n\nprint(f\"Actual number of claims: {df_test['ClaimNb'].sum()}\")\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\nplt.subplots_adjust(wspace=0.3)\n\nfor axi, model in zip(ax.ravel(), [ridge_glm, poisson_glm, dummy]):\n    y_pred = model.predict(df_test)\n    y_true = df_test[\"Frequency\"].values\n    exposure = df_test[\"Exposure\"].values\n    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n        y_true, y_pred, sample_weight=exposure, n_bins=10\n    )\n\n    # Name of the model after the estimator used in the last step of the\n    # pipeline.\n    print(f\"Predicted number of claims by {model[-1]}: {np.sum(y_pred * exposure):.1f}\")\n\n    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n    axi.set_xlim(0, 1.0)\n    axi.set_ylim(0, 0.5)\n    axi.set(\n        title=model[-1],\n        xlabel=\"Fraction of samples sorted by y_pred\",\n        ylabel=\"Mean Frequency (y_pred)\",\n    )\n    axi.legend()\nplt.tight_layout()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:01:17.936319Z","iopub.execute_input":"2022-03-02T07:01:17.936613Z","iopub.status.idle":"2022-03-02T07:01:20.529772Z","shell.execute_reply.started":"2022-03-02T07:01:17.936583Z","shell.execute_reply":"2022-03-02T07:01:20.528780Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import auc\n\n\ndef lorenz_curve(y_true, y_pred, exposure):\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n    exposure = np.asarray(exposure)\n\n    # order samples by increasing predicted risk:\n    ranking = np.argsort(y_pred)\n    ranked_frequencies = y_true[ranking]\n    ranked_exposure = exposure[ranking]\n    cumulated_claims = np.cumsum(ranked_frequencies * ranked_exposure)\n    cumulated_claims /= cumulated_claims[-1]\n    cumulated_exposure = np.cumsum(ranked_exposure)\n    cumulated_exposure /= cumulated_exposure[-1]\n    return cumulated_exposure, cumulated_claims\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor model in [dummy, ridge_glm, poisson_glm]:\n    y_pred = model.predict(df_test)\n    cum_exposure, cum_claims = lorenz_curve(\n        df_test[\"Frequency\"], y_pred, df_test[\"Exposure\"]\n    )\n    gini = 1 - 2 * auc(cum_exposure, cum_claims)\n    label = \"{} (Gini: {:.2f})\".format(model[-1], gini)\n    ax.plot(cum_exposure, cum_claims, linestyle=\"-\", label=label)\n\n# Oracle model: y_pred == y_test\ncum_exposure, cum_claims = lorenz_curve(\n    df_test[\"Frequency\"], df_test[\"Frequency\"], df_test[\"Exposure\"]\n)\ngini = 1 - 2 * auc(cum_exposure, cum_claims)\nlabel = \"Oracle (Gini: {:.2f})\".format(gini)\nax.plot(cum_exposure, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\n\n# Random Baseline\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\nax.set(\n    title=\"Lorenz curves by model\",\n    xlabel=\"Cumulative proportion of exposure (from safest to riskiest)\",\n    ylabel=\"Cumulative proportion of claims\",\n)\nax.legend(loc=\"upper left\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:01:49.552564Z","iopub.execute_input":"2022-03-02T07:01:49.552874Z","iopub.status.idle":"2022-03-02T07:01:52.055940Z","shell.execute_reply.started":"2022-03-02T07:01:49.552841Z","shell.execute_reply":"2022-03-02T07:01:52.054866Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plt.show()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-02T07:02:07.373020Z","iopub.execute_input":"2022-03-02T07:02:07.373329Z","iopub.status.idle":"2022-03-02T07:02:07.378942Z","shell.execute_reply.started":"2022-03-02T07:02:07.373296Z","shell.execute_reply":"2022-03-02T07:02:07.377776Z"},"trusted":true},"execution_count":20,"outputs":[]}]}
